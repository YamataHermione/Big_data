{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/07 13:09:08 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.108.128 instead (on interface ens33)\n",
      "22/05/07 13:09:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/07 13:09:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"mid\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 3, 47260, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_min_sum_count(partition):\n",
    "    is_first = True\n",
    "    sum_ = 0\n",
    "    count = 0\n",
    "    for element in partition:\n",
    "        if(is_first==True):\n",
    "            max_ = element\n",
    "            min_ = element\n",
    "            is_first = False\n",
    "        else:\n",
    "            max_ = max(element, max_)\n",
    "            min_ = min(element, min)\n",
    "        sum_ += element\n",
    "        count += 1\n",
    "    return [(max_, min_, sum_, count)]\n",
    "\n",
    "def min_max_sum_count_map(number):\n",
    "    return (number, number, number, 1)\n",
    "\n",
    "def reducer(pair1, pair2):\n",
    "    return (\n",
    "    max(pair1[0], pair2[0]), \n",
    "    min(pair1[0], pair2[0]), \n",
    "    (pair1[0] + pair2[0]), \n",
    "    (pair1[1] + pair2[1])\n",
    "    )\n",
    "\n",
    "l = [random.randint(1, 1000) for i in range(100)]\n",
    "rdd1 = sc.parallelize(l)\n",
    "final = rdd1.map(min_max_sum_count_map).reduce(lambda a,b: (max(a[0], b[0]), min(a[1],b[1]), (a[2]+b[2]), (a[3] + b[3])))\n",
    "final\n",
    "#final = rdd.mapPartitions(max_min_sum_count).reduce(lambda a, b: (max(a[0], b[0]), min(a[1], b[1]), sum(a[2], b[2]), sum(a[3], b[3])))\n",
    "#print(\"max:\" final[0])\n",
    "#print(\"min:\" final[1])\n",
    "#print(\"min:\" final[2]/final[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zpn(partition):\n",
    "    z, p, n = 0, 0, 0\n",
    "    for element in partition:\n",
    "        if(element>0):\n",
    "            p+=1\n",
    "        elif(element <0):\n",
    "            n+=1\n",
    "        else:\n",
    "            z+=1\n",
    "    return [(z,p,n)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef map(key, value):\\n    emit(key, (value, 1))\\n\\ndef reduce(key, value): key->key (value, 1) -> value\\n    emit(sum(value[0] / sum(value[1])))\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def map(key, value):\n",
    "    emit(key, (value, 1))\n",
    "\n",
    "def reduce(key, value): key->key (values list, list of 1s) -> value\n",
    "    emit(sum(value[0] / sum(value[1])))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "distinct()\n",
    "\n",
    "union()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "joined frame:\n",
    "\n",
    "b1, c1, NaN\n",
    "b1, c2, NaN\n",
    "b2, a2, NaN\n",
    "b7, c1, a1\n",
    "b7, c2, a2\n",
    "\n",
    "\n",
    "def map(record):\n",
    "    rec = record.split(',')\n",
    "    key = rec[1]\n",
    "    value = rec[2]\n",
    "    emit(key, value)\n",
    "\n",
    "def reduce(key: 'b', values: 'a' or 'c'):\n",
    "    \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_Grades = []\n",
    "import random\n",
    "for i in range(100):\n",
    "    id_Grades.append(str(random.randint(1, 10)) + ',' + str(random.randint(1, 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4', (8, 90)),\n",
       " ('9', (50, 100)),\n",
       " ('10', (14, 100)),\n",
       " ('1', (5, 92)),\n",
       " ('8', (3, 97)),\n",
       " ('2', (5, 89)),\n",
       " ('5', (1, 96)),\n",
       " ('3', (13, 92)),\n",
       " ('7', (10, 79)),\n",
       " ('6', (2, 88))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def mapper(record):\n",
    "    rec = record.split(',')\n",
    "    name = rec[0]\n",
    "    grade = int(rec[1])\n",
    "\n",
    "    return (name, grade)\n",
    "\n",
    "def select_min_max(values):\n",
    "    return (min(values), max(values))\n",
    "\n",
    "\n",
    "rdd8 = sc.parallelize(id_Grades)\n",
    "result = rdd8.map(mapper).groupByKey().mapValues(select_min_max).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('you', 'doing'), 2),\n",
       " (('Hello', 'how'), 3),\n",
       " (('how', 'are'), 4),\n",
       " (('are', 'you'), 4),\n",
       " (('Hi', 'how'), 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11 = sc.textFile('11.txt')\n",
    "def mapper(record):\n",
    "    words = record.split(',')\n",
    "    pairs_list = []\n",
    "    for i in range(len(words) - 1):\n",
    "        pairs_list.append((words[i], words[i+1]))\n",
    "    return pairs_list\n",
    "#rdd11.collect()\n",
    "tmp = rdd11.flatMap(mapper).map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b).collect()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(k1, 4)\\n(k2, 2)\\n(k3, 2)\\n(k1, 2)\\n(k2, 2)\\n(k2, 2)\\n(k3, 8)\\n(k3, 6)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "(\"k1\", 4)\n",
    "(k2, 2)\n",
    "(k3, 2)\n",
    "(k1, 2)\n",
    "(k2, 2)\n",
    "(k2, 2)\n",
    "(k3, 8)\n",
    "(k3, 6)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"k1\" : [4, 2]\\n\"k2\" : [2, 2, 2]\\n\"k3\" : [2, 8, 6]\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\"k1\" : [4, 2]\n",
    "\"k2\" : [2, 2, 2]\n",
    "\"k3\" : [2, 8, 6]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n((\"k1\", 6), (\"k2\", 6), (\"k3\", 16))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "((\"k1\", 6), (\"k2\", 6), (\"k3\", 16))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "rdd14 = sc.textFile(\"16\")\n",
    "#rdd14.getNumPartitions()\n",
    "#print(rdd14.collect())\n",
    "def mapper(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    cnt = 0\n",
    "    pcnt = 0\n",
    "    for word in words:\n",
    "        cnt+=1\n",
    "        length = len(word)-1\n",
    "        is_p = True\n",
    "        for i in range(len(word)):\n",
    "            if(word[i] != word[length-i]):\n",
    "                is_p=False\n",
    "                break\n",
    "        if(is_p == True):\n",
    "            pcnt+=1\n",
    "    return (pcnt, cnt)\n",
    "\n",
    "def reducer(pair1, pair2):\n",
    "    return (pair1[0]+pair2[0], pair1[1] + pair2[1])\n",
    "\n",
    "#pair = mapper('wow i thought i did forgot all the stuff you told me')\n",
    "#pair\n",
    "result = rdd14.map(mapper).reduce(lambda a,b: (a[0]+b[0], a[1]+b[1]))\n",
    "print(result[0]/result[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18\n",
    "+ a: format of the output of map: `(\"key\", value),(\"key\", key), (\"key1\",1)/(\"key2\", 2)`\n",
    "+ b: (\"key\", []) (\"key1\", []) (\"key2\", [])\n",
    "+ c: 3\n",
    "+ d: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 19?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def mapper(key: documentNumber, value: document):\n",
    "    words = document.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        emit(words[i], (key, i))\n",
    "\n",
    "def reducer(key:word, values: (documentNumber, i)):\n",
    "    dic = {}\n",
    "    for element in values:\n",
    "        if(element[0] not in dic.keys()):\n",
    "            dic[element[0]] = []\n",
    "        else:\n",
    "            dic[element[0]].append(element[1])\n",
    "    emit(key, [(element: dic[element]) for element in dic.keys()])\n",
    "'''\n",
    "'''\n",
    "#another method\n",
    "def mapper(key:docNumber, value:doc):\n",
    "    words = doc.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        emit((words[i], key), i)\n",
    "\n",
    "def reducer1(key:(word,docNumber), values: indexes):\n",
    "    emit(word, (docNumber, indexes))\n",
    "\n",
    "def reducer2(key: word, values: (docNumber, indexes)):\n",
    "    emit(word, (docNumber, indexes))\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('g1', (2, 6.3999999999999995)), ('g2', (3, 8.5))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd23 = sc.textFile('23.txt')\n",
    "#a\n",
    "#rdd23.count()\n",
    "#b\n",
    "rdd23.map(lambda x: x.split(','))\\\n",
    ".filter(lambda x: x[1]=='1')\\\n",
    ".map(lambda x: (x[0], (int(x[1]), float(x[2]))))\\\n",
    ".reduceByKey(lambda a, b: ((a[0]+ b[0]), (a[1] + b[1])))\\\n",
    ".collect()\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['g1', '1', '2.3'],\n",
       " ['g1', '2', '1.5'],\n",
       " ['g1', '1', '4.1'],\n",
       " ['g2', '1', '1.3'],\n",
       " ['g2', '2', '1.8'],\n",
       " ['g2', '1', '4.3'],\n",
       " ['g2', '1', '2.9']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#c\n",
    "rdd23.map(lambda x: x.split(','))\\\n",
    ".filter(lambda x: x[1]!='3')\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "'''\n",
    "(1,1)\n",
    "(2,2)\n",
    "(3,1)\n",
    "(1,3)\n",
    "(4,4)\n",
    "(2,1)\n",
    "(4,2)\n",
    "(5,5)\n",
    "'''\n",
    "\n",
    "#b\n",
    "'''\n",
    "1, (1, 3)\n",
    "2, (1, 2)\n",
    "3, (1)\n",
    "4, (2, 4)\n",
    "5, (5)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26?\n",
    "what is the meaning of 'efficient' in this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 42?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie10', 3), ('movie16', 5), ('movie4', 2), ('movie18', 3), ('movie8', 3), ('movie1', 2), ('movie11', 5), ('movie15', 2), ('movie6', 7), ('movie2', 5), ('movie13', 4), ('movie14', 3), ('movie7', 5), ('movie12', 8), ('movie3', 2), ('movie20', 4), ('movie9', 4), ('movie5', 5), ('movie19', 5), ('movie17', 1)]\n",
      "+++++++++++++++++++++++\n",
      "[('user6', 7), ('user2', 11), ('user8', 8), ('user5', 7), ('user4', 5), ('user1', 6), ('user9', 7), ('user10', 10), ('user3', 8), ('user7', 9)]\n"
     ]
    }
   ],
   "source": [
    "#create a data file\n",
    "'''\n",
    "with open('42.txt', 'w') as file:\n",
    "    for i in range(100):\n",
    "        file.writelines('user' + str(random.randint(1,10)) + ',movie' + str(random.randint(1,20)) + ',' + str(random.randint(1,5)) + '\\n')\n",
    "'''\n",
    "\n",
    "rdd42 = sc.textFile('42.txt')\n",
    "\n",
    "# MUST use the provided functions.\n",
    "# how to use these fuctions in pyspark program\n",
    "def getUser(record):\n",
    "    seg = record.split(',')\n",
    "    return seg[0]\n",
    "\n",
    "def getMovie(record):\n",
    "    return record.split(',')[1]\n",
    "\n",
    "def getRating(record):\n",
    "    return record.split(',')[2]\n",
    "# a \n",
    "def generateMovieUsrPairs(record):\n",
    "    return (getMovie(record), getUser(record))\n",
    "\n",
    "def generateUsrMoviePairs(record):\n",
    "    return (getUser(record), getMovie(record))\n",
    "# a\n",
    "print(rdd42.map(generateMovieUsrPairs).groupByKey().mapValues(lambda X: len(set(X))).collect())\n",
    "print('+++++++++++++++++++++++')\n",
    "#b\n",
    "print(rdd42.map(generateUsrMoviePairs).groupByKey().mapValues(lambda X: len(set(X))).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "'''\n",
    "def map(key: alphabet, value: list of numbers):\n",
    "    emit(key, max(value)) # return key and local maximum\n",
    "'''\n",
    "\n",
    "#b\n",
    "'''\n",
    "a, 50\n",
    "a, 60\n",
    "b, 50\n",
    "b, 55\n",
    "'''\n",
    "#c\n",
    "'''\n",
    "def reduce(key: alphabet, value : list of local maximums):\n",
    "    emit(key, max(value)) # global maximum\n",
    "'''\n",
    "#d\n",
    "'''\n",
    "a, (50, 60)\n",
    "b, (50, 55)\n",
    "'''\n",
    "#e\n",
    "'''\n",
    "2\n",
    "'''\n",
    "\n",
    "#f\n",
    "'''\n",
    "a, 50\n",
    "b, 55\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abc', 'xyz']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [\"abc\", \"abc\", \"xyz\", \"xyz\", \"xyz\"]\n",
    "rdd50 = sc.parallelize(data)\n",
    "\n",
    "rdd50.map(lambda x: (x, 1))\\\n",
    ".reduceByKey(lambda a, b: (a + b))\\\n",
    ".map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-3, 3, 3, -4, 4, 4]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfunc(n):\n",
    "    if n < 0:\n",
    "        return [n, -n, -n]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "data = [0, 1, 2, -3, -4]\n",
    "rdd54 = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd54.flatMap(myfunc).flatMap(myfunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 55 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef map(key: <key-as-string>, value: <value-as-int>):\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def map(key: <key-as-string>, value: <value-as-int>):\n",
    "    emit(key, value)\n",
    "\n",
    "\n",
    "def reduce(key: <key-as-string>, value: list of values with same key):\n",
    "    value = sorted(value)\n",
    "    if len(value) % 2==0:\n",
    "        emit(key, mean(value[int(len(value)/2)], value[int(len(value)/2) + 1]))\n",
    "    else:\n",
    "        emit(key, value[int(len(value)/2) + 1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b', 'b', 'b', 'd', 'd', 'x', 'x', 'x', 'y']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [('a', 2), ('b', 3), ('d', 2),('x', 3), ('y', 1)]\n",
    "rdd66 = sc.parallelize(data)\n",
    "\n",
    "def dup(pair):\n",
    "    return [pair[0] for i in range(pair[1])]\n",
    "\n",
    "rdd66.flatMap(dup).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nE\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "E\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 71 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 73?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what does 'efficient' mean?\n",
    "data = [1, 11, -1, 2, 12, 3, -4, 13, 4, 14]\n",
    "rdd73 = sc.parallelize(data)\n",
    "def countOddEven(number):\n",
    "    if((number >0) and (number % 2 ==0)):\n",
    "        return (0, 1)\n",
    "    elif((number > 0) and (number % 2 != 0)):\n",
    "        return (1, 0)\n",
    "    else:\n",
    "        return (0, 0)\n",
    "\n",
    "rdd73.map(countOddEven).reduce(lambda x, y: ((x[0] + y[0]), (x[1] + y[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
