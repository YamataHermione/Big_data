{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/23 16:27:51 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.108.128 instead (on interface ens33)\n",
      "22/05/23 16:27:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/23 16:27:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkDataFrameDemo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Serial No.='1', GRE Score='337', TOEFL Score='118', University Rating='4', SOP='4.5', LOR ='4.5', CGPA='9.65', Research='1', Chance of Admit ='0.92'),\n",
       " Row(Serial No.='2', GRE Score='324', TOEFL Score='107', University Rating='4', SOP='4', LOR ='4.5', CGPA='8.87', Research='1', Chance of Admit ='0.76'),\n",
       " Row(Serial No.='3', GRE Score='316', TOEFL Score='104', University Rating='3', SOP='3', LOR ='3.5', CGPA='8', Research='1', Chance of Admit ='0.72'),\n",
       " Row(Serial No.='4', GRE Score='322', TOEFL Score='110', University Rating='3', SOP='3.5', LOR ='2.5', CGPA='8.67', Research='1', Chance of Admit ='0.8'),\n",
       " Row(Serial No.='5', GRE Score='314', TOEFL Score='103', University Rating='2', SOP='2', LOR ='3', CGPA='8.21', Research='0', Chance of Admit ='0.65')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = \"Admission_Predict.csv\"\n",
    "df = spark.read.option('header', True).csv(input_path)\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Serial No.: string (nullable = true)\n",
      " |-- GRE Score: string (nullable = true)\n",
      " |-- TOEFL Score: string (nullable = true)\n",
      " |-- University Rating: string (nullable = true)\n",
      " |-- SOP: string (nullable = true)\n",
      " |-- LOR : string (nullable = true)\n",
      " |-- CGPA: string (nullable = true)\n",
      " |-- Research: string (nullable = true)\n",
      " |-- Chance of Admit : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit |\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|\n",
      "|         2|      324|        107|                4|  4| 4.5|8.87|       1|            0.76|\n",
      "|         3|      316|        104|                3|  3| 3.5|   8|       1|            0.72|\n",
      "|         4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|\n",
      "|         5|      314|        103|                2|  2|   3|8.21|       0|            0.65|\n",
      "|         6|      330|        115|                5|4.5|   3|9.34|       1|             0.9|\n",
      "|         7|      321|        109|                3|  3|   4| 8.2|       1|            0.75|\n",
      "|         8|      308|        101|                2|  3|   4| 7.9|       0|            0.68|\n",
      "|         9|      302|        102|                1|  2| 1.5|   8|       0|             0.5|\n",
      "|        10|      323|        108|                3|3.5|   3| 8.6|       0|            0.45|\n",
      "|        11|      325|        106|                3|3.5|   4| 8.4|       1|            0.52|\n",
      "|        12|      327|        111|                4|  4| 4.5|   9|       1|            0.84|\n",
      "|        13|      328|        112|                4|  4| 4.5| 9.1|       1|            0.78|\n",
      "|        14|      307|        109|                3|  4|   3|   8|       1|            0.62|\n",
      "|        15|      311|        104|                3|3.5|   2| 8.2|       1|            0.61|\n",
      "|        16|      314|        105|                3|3.5| 2.5| 8.3|       0|            0.54|\n",
      "|        17|      317|        107|                3|  4|   3| 8.7|       0|            0.66|\n",
      "|        18|      319|        106|                3|  4|   3|   8|       1|            0.65|\n",
      "|        19|      318|        110|                3|  4|   3| 8.8|       0|            0.63|\n",
      "|        20|      303|        102|                3|3.5|   3| 8.5|       0|            0.62|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"Graduate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit |\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|\n",
      "|         2|      324|        107|                4|  4| 4.5|8.87|       1|            0.76|\n",
      "|         3|      316|        104|                3|  3| 3.5|   8|       1|            0.72|\n",
      "|         4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|\n",
      "|         6|      330|        115|                5|4.5|   3|9.34|       1|             0.9|\n",
      "|         7|      321|        109|                3|  3|   4| 8.2|       1|            0.75|\n",
      "|        11|      325|        106|                3|3.5|   4| 8.4|       1|            0.52|\n",
      "|        12|      327|        111|                4|  4| 4.5|   9|       1|            0.84|\n",
      "|        13|      328|        112|                4|  4| 4.5| 9.1|       1|            0.78|\n",
      "|        14|      307|        109|                3|  4|   3|   8|       1|            0.62|\n",
      "|        15|      311|        104|                3|3.5|   2| 8.2|       1|            0.61|\n",
      "|        18|      319|        106|                3|  4|   3|   8|       1|            0.65|\n",
      "|        21|      312|        107|                3|  3|   2| 7.9|       1|            0.64|\n",
      "|        23|      328|        116|                5|  5|   5| 9.5|       1|            0.94|\n",
      "|        24|      334|        119|                5|  5| 4.5| 9.7|       1|            0.95|\n",
      "|        25|      336|        119|                5|  4| 3.5| 9.8|       1|            0.97|\n",
      "|        26|      340|        120|                5|4.5| 4.5| 9.6|       1|            0.94|\n",
      "|        28|      298|         98|                2|1.5| 2.5| 7.5|       1|            0.44|\n",
      "|        31|      300|         97|                2|  3|   3| 8.1|       1|            0.65|\n",
      "|        32|      327|        103|                3|  4|   4| 8.3|       1|            0.74|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.sql(\"SELECT * FROM Graduate WHERE Research=1\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can drop the temprory view\n",
    "spark.catalog.dropTempView(\"Graduate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import exp, lit \n",
    "df_with_nc1 = df.withColumn(\"n1\", lit(0))\n",
    "df_with_nc2 = df_with_nc1.withColumn(\"n2\", exp(\"CGPA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit | n1|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+\n",
      "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|  0|\n",
      "|         2|      324|        107|                4|  4| 4.5|8.87|       1|            0.76|  0|\n",
      "|         3|      316|        104|                3|  3| 3.5|   8|       1|            0.72|  0|\n",
      "|         4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|  0|\n",
      "|         5|      314|        103|                2|  2|   3|8.21|       0|            0.65|  0|\n",
      "|         6|      330|        115|                5|4.5|   3|9.34|       1|             0.9|  0|\n",
      "|         7|      321|        109|                3|  3|   4| 8.2|       1|            0.75|  0|\n",
      "|         8|      308|        101|                2|  3|   4| 7.9|       0|            0.68|  0|\n",
      "|         9|      302|        102|                1|  2| 1.5|   8|       0|             0.5|  0|\n",
      "|        10|      323|        108|                3|3.5|   3| 8.6|       0|            0.45|  0|\n",
      "|        11|      325|        106|                3|3.5|   4| 8.4|       1|            0.52|  0|\n",
      "|        12|      327|        111|                4|  4| 4.5|   9|       1|            0.84|  0|\n",
      "|        13|      328|        112|                4|  4| 4.5| 9.1|       1|            0.78|  0|\n",
      "|        14|      307|        109|                3|  4|   3|   8|       1|            0.62|  0|\n",
      "|        15|      311|        104|                3|3.5|   2| 8.2|       1|            0.61|  0|\n",
      "|        16|      314|        105|                3|3.5| 2.5| 8.3|       0|            0.54|  0|\n",
      "|        17|      317|        107|                3|  4|   3| 8.7|       0|            0.66|  0|\n",
      "|        18|      319|        106|                3|  4|   3|   8|       1|            0.65|  0|\n",
      "|        19|      318|        110|                3|  4|   3| 8.8|       0|            0.63|  0|\n",
      "|        20|      303|        102|                3|3.5|   3| 8.5|       0|            0.62|  0|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_nc1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+------------------+\n",
      "|Serial No.|GRE Score|TOEFL Score|University Rating|SOP|LOR |CGPA|Research|Chance of Admit | n1|                n2|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+------------------+\n",
      "|         1|      337|        118|                4|4.5| 4.5|9.65|       1|            0.92|  0|15521.788104196934|\n",
      "|         2|      324|        107|                4|  4| 4.5|8.87|       1|            0.76|  0| 7115.280973169776|\n",
      "|         3|      316|        104|                3|  3| 3.5|   8|       1|            0.72|  0|2980.9579870417283|\n",
      "|         4|      322|        110|                3|3.5| 2.5|8.67|       1|             0.8|  0| 5825.499349524731|\n",
      "|         5|      314|        103|                2|  2|   3|8.21|       0|            0.65|  0|3677.5424662662012|\n",
      "|         6|      330|        115|                5|4.5|   3|9.34|       1|             0.9|  0|11384.408240181616|\n",
      "|         7|      321|        109|                3|  3|   4| 8.2|       1|            0.75|  0| 3640.950307332352|\n",
      "|         8|      308|        101|                2|  3|   4| 7.9|       0|            0.68|  0|  2697.28232826851|\n",
      "|         9|      302|        102|                1|  2| 1.5|   8|       0|             0.5|  0|2980.9579870417283|\n",
      "|        10|      323|        108|                3|3.5|   3| 8.6|       0|            0.45|  0| 5431.659591362979|\n",
      "|        11|      325|        106|                3|3.5|   4| 8.4|       1|            0.52|  0| 4447.066747699858|\n",
      "|        12|      327|        111|                4|  4| 4.5|   9|       1|            0.84|  0| 8103.083927575384|\n",
      "|        13|      328|        112|                4|  4| 4.5| 9.1|       1|            0.78|  0| 8955.292703482508|\n",
      "|        14|      307|        109|                3|  4|   3|   8|       1|            0.62|  0|2980.9579870417283|\n",
      "|        15|      311|        104|                3|3.5|   2| 8.2|       1|            0.61|  0| 3640.950307332352|\n",
      "|        16|      314|        105|                3|3.5| 2.5| 8.3|       0|            0.54|  0| 4023.872393822313|\n",
      "|        17|      317|        107|                3|  4|   3| 8.7|       0|            0.66|  0| 6002.912217261018|\n",
      "|        18|      319|        106|                3|  4|   3|   8|       1|            0.65|  0|2980.9579870417283|\n",
      "|        19|      318|        110|                3|  4|   3| 8.8|       0|            0.63|  0|  6634.24400627789|\n",
      "|        20|      303|        102|                3|3.5|   3| 8.5|       0|            0.62|  0| 4914.768840299134|\n",
      "+----------+---------+-----------+-----------------+---+----+----+--------+----------------+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_nc2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate multiple columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, expr, col\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType, StructField, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.sparkContext.parallelize([\\\n",
    "        (\"mary\", \"lemon\", 2.00),\\\n",
    "        (\"adam\", \"grape\", 1.22),\\\n",
    "        (\"adam\", \"carrot\", 2.44),\\\n",
    "        (\"adam\", \"orange\", 1.99),\\\n",
    "        (\"john\", \"tomato\", 1.99),\\\n",
    "        (\"john\", \"carrot\", 0.45),\\\n",
    "        (\"john\", \"banana\", 1.29),\\\n",
    "        (\"bill\", \"apple\", 0.99),\\\n",
    "        (\"bill\", \"taco\", 2.59)\\\n",
    "        ]).toDF([\"name\", \"food\", \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|name|  food|price|\n",
      "+----+------+-----+\n",
      "|mary| lemon|  2.0|\n",
      "|adam| grape| 1.22|\n",
      "|adam|carrot| 2.44|\n",
      "|adam|orange| 1.99|\n",
      "|john|tomato| 1.99|\n",
      "|john|carrot| 0.45|\n",
      "|john|banana| 1.29|\n",
      "|bill| apple| 0.99|\n",
      "|bill|  taco| 2.59|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+------------------+\n",
      "|name|food                    |price             |\n",
      "+----+------------------------+------------------+\n",
      "|adam|[grape, carrot, orange] |[1.22, 2.44, 1.99]|\n",
      "|mary|[lemon]                 |[2.0]             |\n",
      "|john|[tomato, carrot, banana]|[1.99, 0.45, 1.29]|\n",
      "|bill|[apple, taco]           |[0.99, 2.59]      |\n",
      "+----+------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('name').agg(expr('collect_list(food) as food'), expr('collect_list(price) as price')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+------------------+------------------------------------------------+\n",
      "|name|food                    |price             |x                                               |\n",
      "+----+------------------------+------------------+------------------------------------------------+\n",
      "|adam|[grape, carrot, orange] |[1.22, 2.44, 1.99]|[{grape, 1.22}, {carrot, 2.44}, {orange, 1.99}] |\n",
      "|mary|[lemon]                 |[2.0]             |[{lemon, 2.0}]                                  |\n",
      "|john|[tomato, carrot, banana]|[1.99, 0.45, 1.29]|[{tomato, 1.99}, {carrot, 0.45}, {banana, 1.29}]|\n",
      "|bill|[apple, taco]           |[0.99, 2.59]      |[{apple, 0.99}, {taco, 2.59}]                   |\n",
      "+----+------------------------+------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf(\"array<struct<_1:string, _2:double>>\")\n",
    "def zip_(xs, ys):\n",
    "    return list(zip(xs, ys))\n",
    "df.groupBy('name').agg(expr('collect_list(food) as food'), expr('collect_list(price) as price')).withColumn('x', zip_(col('food'), col('price'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|name|avg(price)        |\n",
      "+----+------------------+\n",
      "|adam|1.8833333333333335|\n",
      "|mary|2.0               |\n",
      "|john|1.2433333333333334|\n",
      "|bill|1.79              |\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We can also do operations in a pandas-like method\n",
    "#However it seems that we can only apply one function to the grouped object\n",
    "df.groupBy('name').agg({'price': 'max', 'price':'min', 'price': 'mean'}).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------------------+\n",
      "|name|min(price)|max(price)|        avg(price)|\n",
      "+----+----------+----------+------------------+\n",
      "|adam|      1.22|      2.44|1.8833333333333335|\n",
      "|mary|       2.0|       2.0|               2.0|\n",
      "|john|      0.45|      1.99|1.2433333333333334|\n",
      "|bill|      0.99|      2.59|              1.79|\n",
      "+----+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, avg\n",
    "funcs = [min, max, avg]\n",
    "to_agg = [\"price\"]\n",
    "exprs = [f(col(name)) for f in funcs for name in to_agg]\n",
    "df.groupBy('name').agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow) (1.22.3)\n",
      "Installing collected packages: pyarrow\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/shijuzheng/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed pyarrow-8.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|name|upper(name)|\n",
      "+----+-----------+\n",
      "|adam|       ADAM|\n",
      "|mary|       MARY|\n",
      "|john|       JOHN|\n",
      "|bill|       BILL|\n",
      "+----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from pyspark.sql.functions import pandas_udf\n",
    "@pandas_udf('string')\n",
    "def upper(x: pd.Series) -> pd.Series:\n",
    "    return x.str.upper()\n",
    "\n",
    "df.groupBy('name').agg(upper(col('name'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|name|price|     squared_price|\n",
      "+----+-----+------------------+\n",
      "|mary|  2.0|               4.0|\n",
      "|adam| 1.22|            1.4884|\n",
      "|adam| 2.44|            5.9536|\n",
      "|adam| 1.99|            3.9601|\n",
      "|john| 1.99|            3.9601|\n",
      "|john| 0.45|            0.2025|\n",
      "|john| 1.29|1.6641000000000001|\n",
      "|bill| 0.99|            0.9801|\n",
      "|bill| 2.59| 6.708099999999999|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "squared_udf = udf(squared, DoubleType())\n",
    "df.select('name', 'price', squared_udf(col('price')).alias('squared_price')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame from Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept1= Row(id='100', name='Computer Science')\n",
      "employee2= Row(first_name='jane', last_name='goldman', email='jane@stanford.edu', salary=120000)\n",
      "department_with_employees_1 Row(department=Row(id='100', name='Computer Science'), employees=[Row(first_name='alex', last_name='smith', email='alex@berkeley.edu', salary=110000), Row(first_name='jane', last_name='goldman', email='jane@stanford.edu', salary=120000), Row(first_name='betty', last_name='ford', email='betty@gmail.com', salary=130000)])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "dept1 = Row(id='100', name='Computer Science')\n",
    "dept2 = Row(id='200', name='Mechanical Engineering')\n",
    "dept3 = Row(id='300', name='Music')\n",
    "dept4 = Row(id='400', name='Sports')\n",
    "dept5 = Row(id='500', name='Biology')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"first_name\", \"last_name\", \"email\", \"salary\")\n",
    "\n",
    "employee1 = Employee('alex', 'smith', 'alex@berkeley.edu', 110000)\n",
    "employee2 = Employee('jane', 'goldman', 'jane@stanford.edu', 120000)\n",
    "employee3 = Employee('matei', None, 'matei@yahoo.com', 140000)\n",
    "employee4 = Employee(None, 'eastwood', 'jimmy@berkeley.edu', 160000)\n",
    "employee5 = Employee('betty', 'ford', 'betty@gmail.com', 130000)\n",
    "\n",
    "department_with_employees_1 = Row(department=dept1, employees = [employee1, employee2, employee5])\n",
    "department_with_employees_2 = Row(department=dept2, employees=[employee3, employee4])\n",
    "department_with_employees_3 = Row(department=dept3, employees=[employee1, employee4])\n",
    "department_with_employees_4 = Row(department=dept4, employees=[employee2, employee3])\n",
    "department_with_employees_5 = Row(department=dept5, employees=[employee5])\n",
    "\n",
    "\n",
    "print(\"dept1=\", dept1)\n",
    "print(\"employee2=\", employee2)\n",
    "print(\"department_with_employees_1\", department_with_employees_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          department|           employees|\n",
      "+--------------------+--------------------+\n",
      "|{100, Computer Sc...|[{alex, smith, al...|\n",
      "|{200, Mechanical ...|[{matei, null, ma...|\n",
      "|      {500, Biology}|[{betty, ford, be...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "departments_with_employees_seq_1 = [department_with_employees_1, department_with_employees_2, department_with_employees_5]\n",
    "spark.createDataFrame(departments_with_employees_seq_1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- department: struct (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- employees: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- first_name: string (nullable = true)\n",
      " |    |    |-- last_name: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(departments_with_employees_seq_1).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|name|  food|price|\n",
      "+----+------+-----+\n",
      "|adam| grape| 1.22|\n",
      "|adam|orange| 1.99|\n",
      "|adam|carrot| 2.44|\n",
      "|bill|  taco| 2.59|\n",
      "|bill| apple| 0.99|\n",
      "|john|carrot| 0.45|\n",
      "|john|tomato| 1.99|\n",
      "|john|banana| 1.29|\n",
      "|mary| lemon|  2.0|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|name|  food|price|\n",
      "+----+------+-----+\n",
      "|bill| apple| 0.99|\n",
      "|john|banana| 1.29|\n",
      "|john|carrot| 0.45|\n",
      "|adam|carrot| 2.44|\n",
      "|adam| grape| 1.22|\n",
      "|mary| lemon|  2.0|\n",
      "|adam|orange| 1.99|\n",
      "|bill|  taco| 2.59|\n",
      "|john|tomato| 1.99|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df['food']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|name|  food|price|\n",
      "+----+------+-----+\n",
      "|bill|  taco| 2.59|\n",
      "|adam|carrot| 2.44|\n",
      "|mary| lemon|  2.0|\n",
      "|john|tomato| 1.99|\n",
      "|adam|orange| 1.99|\n",
      "|john|banana| 1.29|\n",
      "|adam| grape| 1.22|\n",
      "|bill| apple| 0.99|\n",
      "|john|carrot| 0.45|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(df.price.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+------------------+\n",
      "|summary|name|  food|             price|\n",
      "+-------+----+------+------------------+\n",
      "|  count|   9|     9|                 9|\n",
      "|   mean|null|  null|1.6622222222222223|\n",
      "| stddev|null|  null|0.7115261375694109|\n",
      "|    min|adam| apple|              0.45|\n",
      "|    25%|null|  null|              1.22|\n",
      "|    50%|null|  null|              1.99|\n",
      "|    75%|null|  null|               2.0|\n",
      "|    max|mary|tomato|              2.59|\n",
      "+-------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: [0.45, 0.45]\n"
     ]
    }
   ],
   "source": [
    "median = df.approxQuantile(\"price\", [0.02, 0.04], 0.01)\n",
    "print(\"median:\", median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple dims Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\\\n",
    "        ('Ames', 2006, 100),\\\n",
    "        ('Ames', 2007, 200),\\\n",
    "        ('Ames', 2008, 300),\\\n",
    "        ('Sunnyvale', 2007, 10),\\\n",
    "        ('Sunnyvale', 2008, 20),\\\n",
    "        ('Sunnyvale', 2009, 30),\\\n",
    "        ('Stanford', 2008, 90)\\\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+------+\n",
      "|     city|year|amount|\n",
      "+---------+----+------+\n",
      "|     Ames|2006|   100|\n",
      "|     Ames|2007|   200|\n",
      "|     Ames|2008|   300|\n",
      "|Sunnyvale|2007|    10|\n",
      "|Sunnyvale|2008|    20|\n",
      "|Sunnyvale|2009|    30|\n",
      "| Stanford|2008|    90|\n",
      "+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['city', 'year', 'amount']\n",
    "sales = spark.createDataFrame(data, columns)\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shijuzheng/Big_data/Module07_SparkDataFrame/SparkDataFrames.ipynb Cell 38'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/shijuzheng/Big_data/Module07_SparkDataFrame/SparkDataFrames.ipynb#ch0000048?line=0'>1</a>\u001b[0m groupby_city_and_year \u001b[39m=\u001b[39m sales\u001b[39m.\u001b[39mgroupBy([\u001b[39m'\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39magg(\u001b[39msum\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mamount\u001b[39;49m\u001b[39m'\u001b[39;49m))\u001b[39m.\u001b[39malias(\u001b[39m'\u001b[39m\u001b[39mamount\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shijuzheng/Big_data/Module07_SparkDataFrame/SparkDataFrames.ipynb#ch0000048?line=1'>2</a>\u001b[0m groupby_city_and_year\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "groupby_city_and_year = sales.groupBy(['city', 'year']).agg(sum('amount')).alias('amount')\n",
    "groupby_city_and_year.show()\n",
    "\n",
    "## maybe because of city is string and year is int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
